{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import inspect\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim.lr_scheduler as lrs\n",
    "import torch_geometric.transforms as T\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.callbacks as plc\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torchmetrics.functional.classification import (\n",
    "    binary_accuracy,\n",
    "    binary_auroc,\n",
    "    binary_f1_score,\n",
    "    multiclass_accuracy,\n",
    "    multiclass_auroc,\n",
    "    multiclass_f1_score,\n",
    ")\n",
    "from utils import find_latest_best_checkpoint  # Ensure this module is available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Training and Data Interface Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train(pl.LightningModule):\n",
    "    def __init__(self, **kargs):\n",
    "        super().__init__()\n",
    "        self.retrain = False\n",
    "        self.save_hyperparameters()\n",
    "        self.load_model()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        batch = batch[0]\n",
    "        out = self.model(batch)\n",
    "        if self.hparams.model_name == 'caf':\n",
    "            loss = self.model.loss(out, batch, mode='train', retrain=self.retrain, epoch=self.current_epoch, retrain_config=self.hparams.retrain_config)\n",
    "        else:\n",
    "            loss = self.model.loss(out, batch, mode='train')\n",
    "        metrics = self.metrics(out, batch, mode='train')\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=False, batch_size=1)\n",
    "        self.log_dict(metrics, on_step=False, on_epoch=True, prog_bar=False, batch_size=1)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        batch = batch[0]\n",
    "        out = self.model(batch)\n",
    "        if self.hparams.model_name == 'caf':\n",
    "            loss = self.model.loss(out, batch, mode='val', retrain=self.retrain, epoch=self.current_epoch, retrain_config=self.hparams.retrain_config)                                \n",
    "        else:\n",
    "            loss = self.model.loss(out, batch, mode='val')\n",
    "        metrics = self.metrics(out, batch, mode='val')\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, batch_size=1)\n",
    "        self.log_dict(metrics, on_step=False, on_epoch=True, prog_bar=False, batch_size=1)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        batch = batch[0]\n",
    "        out = self.model(batch)\n",
    "        if self.hparams.model_name == 'caf':\n",
    "            loss = self.model.loss(out, batch, mode='test', retrain=self.retrain, epoch=self.current_epoch, retrain_config=self.hparams.retrain_config)\n",
    "        else:\n",
    "            loss = self.model.loss(out, batch, mode='test')\n",
    "        metrics = self.metrics(out, batch, mode='test')\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=False, batch_size=1)\n",
    "        self.log_dict(metrics, on_step=False, on_epoch=True, prog_bar=False, batch_size=1)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        if 'weight_decay' in  self.hparams.model_config.keys():\n",
    "            weight_decay = self.hparams.model_config['weight_decay'] if self.retrain == False else self.hparams.retrain_config['weight_decay']\n",
    "        else:\n",
    "            weight_decay = 0\n",
    "        lr = self.hparams.model_config['lr'] if self.retrain == False else self.hparams.retrain_config['lr']\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        if self.hparams.model_config['lr_scheduler'] == None:\n",
    "            return optimizer\n",
    "        else:\n",
    "            if self.hparams.model_config['lr_scheduler'] == 'step':\n",
    "                scheduler = lrs.StepLR(optimizer,\n",
    "                                       step_size=self.hparams.model_config['lr_decay_steps'],\n",
    "                                       gamma=self.hparams.model_config['lr_decay_rate'])\n",
    "            elif self.hparams.model_config['lr_scheduler'] == 'cosine':\n",
    "                scheduler = lrs.CosineAnnealingLR(optimizer,\n",
    "                                                  T_max=self.hparams.model_config['lr_decay_steps'],\n",
    "                                                  eta_min=self.hparams.model_config['lr_decay_min_lr'])\n",
    "            else:\n",
    "                raise ValueError('Invalid lr_scheduler type!')\n",
    "            return [optimizer], [scheduler]\n",
    "\n",
    "    def load_model(self):\n",
    "        name = self.hparams.model_name\n",
    "        upper_name = name.upper()\n",
    "        try:\n",
    "            sys.path.append('./src/models')\n",
    "            Model = getattr(importlib.import_module(name), upper_name)\n",
    "        except:\n",
    "            raise ValueError(f'Invalid Module File Name or Invalid Class Name {name}.{upper_name}!')\n",
    "        self.model = self.instancialize(Model)\n",
    "\n",
    "    def instancialize(self, Model, **other_args):\n",
    "        class_args = inspect.getfullargspec(Model.__init__).args[1:]\n",
    "        inkeys = self.hparams.model_config.keys()\n",
    "        args1 = {}\n",
    "        for arg in class_args:\n",
    "            if arg in inkeys:\n",
    "                args1[arg] = self.hparams.model_config[arg]\n",
    "        args1.update(other_args)\n",
    "        return Model(**args1)\n",
    "    \n",
    "    def metrics(self, out, batch, mode):\n",
    "        preds = out if self.hparams.model_name != 'caf' else out[0]\n",
    "        labels, sens, mask = batch['y'], batch['sens'], batch[f'{mode}_mask']\n",
    "        if self.hparams.model_config['out_dim'] == 1:\n",
    "            acc = binary_accuracy(preds[mask], labels[mask])\n",
    "            f1 = binary_f1_score(preds[mask], labels[mask])\n",
    "            auroc = binary_auroc(preds[mask], labels[mask])\n",
    "        elif self.hparams.model_config['out_dim'] > 2:\n",
    "            acc = multiclass_accuracy(preds[mask], labels[mask], num_classes=self.hparams.out_dim, average='micro')\n",
    "            f1 = multiclass_f1_score(preds[mask], labels[mask], num_classes=self.hparams.out_dim, average='micro')\n",
    "            auroc = multiclass_auroc(preds[mask], labels[mask], num_classes=self.hparams.out_dim, average='macro')\n",
    "        parity, equality = self.binary_fair_metrics(preds[mask], labels[mask], sens[mask])\n",
    "        fair = acc + f1 + auroc - self.hparams.alpha * (parity + equality)\n",
    "        return {f'{mode}_acc': acc, f'{mode}_f1': f1, f'{mode}_auroc': auroc, f'{mode}_parity': parity, \\\n",
    "                f'{mode}_equality': equality, f'{mode}_fair': fair}\n",
    "    \n",
    "    def binary_fair_metrics(self, preds, labels, sens):\n",
    "        idx_s0 = sens==0\n",
    "        idx_s1 = sens==1\n",
    "        idx_s0 = idx_s0.detach().cpu().numpy()\n",
    "        idx_s1 = idx_s1.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        idx_s0_y1 = np.bitwise_and(idx_s0, labels==1)\n",
    "        idx_s1_y1 = np.bitwise_and(idx_s1, labels==1)\n",
    "        preds = (preds.squeeze()>0.5)\n",
    "        parity = abs(sum(preds[idx_s0])/sum(idx_s0)-sum(preds[idx_s1])/sum(idx_s1))\n",
    "        equality = abs(sum(preds[idx_s0_y1])/sum(idx_s0_y1)-sum(preds[idx_s1_y1])/sum(idx_s1_y1))\n",
    "        return parity.item(), equality.item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define dataset interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class DInterface(pl.LightningDataModule):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(logger=False)\n",
    "        self.load_data_module()\n",
    "        \n",
    "    def setup(self, stage):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.trainset = self.dataset\n",
    "            self.valset = self.dataset\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.testset = self.dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.trainset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.testset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    def load_data_module(self):\n",
    "        name = self.hparams.dataset_name\n",
    "        camel_name = ''.join([i.capitalize() for i in name.split('_')])\n",
    "        try:\n",
    "            sys.path.append('./src/datasets')\n",
    "            Dataset = getattr(importlib.import_module(name), camel_name)\n",
    "        except:\n",
    "            raise ValueError(f'Invalid Dataset File Name or Invalid Class Name {name}.{camel_name}')\n",
    "        self.dataset = self.instancialize(Dataset)\n",
    "        \n",
    "    def instancialize(self, Dataset, **other_args):\n",
    "        class_args = inspect.getfullargspec(Dataset.__init__).args[1:]\n",
    "        inkeys = self.hparams.data_config.keys()\n",
    "        args1 = {}\n",
    "        for arg in class_args:\n",
    "            if arg in inkeys:\n",
    "                args1[arg] = self.hparams.data_config[arg]\n",
    "        args1.update(other_args)\n",
    "        if args1['transform'] == 'normalize':\n",
    "            args1['transform'] = T.NormalizeFeatures()\n",
    "        return Dataset(**args1)\n",
    "    \n",
    "    def get_in_out_dim(self):\n",
    "        feat_dim = self.dataset.num_features\n",
    "        class_num = self.dataset.num_classes\n",
    "        print(f'Feature dimension: {feat_dim}')\n",
    "        print(f'Number of classes: {class_num}')\n",
    "        return feat_dim, class_num"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callbacks Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_callbacks(args):\n",
    "    callbacks = []\n",
    "    callbacks.append(plc.ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        filename='{epoch}-best', \n",
    "        save_top_k=1,\n",
    "        mode='min',\n",
    "        save_last=True\n",
    "    ))\n",
    "\n",
    "    callbacks.append(plc.RichProgressBar(\n",
    "        refresh_rate=1\n",
    "    ))\n",
    "\n",
    "    if args.model_config['lr_scheduler'] != None:\n",
    "        callbacks.append(plc.LearningRateMonitor(\n",
    "            logging_interval='epoch'))\n",
    "    return callbacks\n",
    "\n",
    "def load_retrain_callbacks(args):\n",
    "    callbacks = []\n",
    "    callbacks.append(plc.ModelCheckpoint(\n",
    "        monitor='val_fair',\n",
    "        filename='{epoch}-best', \n",
    "        save_top_k=1,\n",
    "        mode='max',\n",
    "        save_last=True\n",
    "    ))\n",
    "\n",
    "    callbacks.append(plc.RichProgressBar(\n",
    "        refresh_rate=1\n",
    "    ))\n",
    "\n",
    "    if args.model_config['lr_scheduler'] != None:\n",
    "        callbacks.append(plc.LearningRateMonitor(\n",
    "            logging_interval='epoch'))\n",
    "    return callbacks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset_name = 'german'  # Specify your dataset name\n",
    "model_name = 'caf'  # Specify your model name\n",
    "seed = 42  # Specify your seed\n",
    "no_train = False  # Specify your no_train\n",
    "\n",
    "# Note: Replace this with actual paths and configurations\n",
    "config_dir = Path('./src/configs')\n",
    "global_config = yaml.safe_load((config_dir / 'global_config.yml').open('r'))\n",
    "local_config = yaml.safe_load((config_dir / f'{dataset_name}_{model_name}.yml').open('r'))\n",
    "\n",
    "# merging configs into a single dictionary and making it a namespace\n",
    "args = argparse.Namespace(**{**global_config, **local_config, **vars(args)})\n",
    "\n",
    "pl.seed_everything(seed)\n",
    "data_module = DInterface(**vars(args))\n",
    "feat_dim, class_num = data_module.get_in_out_dim()\n",
    "args.model_config['in_dim'] = feat_dim\n",
    "args.model_config['out_dim'] = class_num if class_num > 2 else 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = Train(**vars(args))\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "if not no_train:\n",
    "    csv_logger = CSVLogger(save_dir=Path(args.log_dir) / f'{dataset_name}_{model_name}', version=timestamp)\n",
    "    callbacks = load_callbacks(args)\n",
    "    trainer = Trainer(max_epochs=args.epochs, accelerator='gpu',\\\n",
    "                        logger=csv_logger, log_every_n_steps=1, callbacks=callbacks)\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "    trainer.test(model, datamodule=data_module, ckpt_path='best')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retraining (if using the 'caf' model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if model_name == 'caf':\n",
    "    best_checkpoint = find_latest_best_checkpoint(Path(args.log_dir) / f'{dataset_name}_{model_name}')\n",
    "    best_model_path = best_checkpoint if no_train else trainer.checkpoint_callback.best_model_path\n",
    "    model = Train.load_from_checkpoint(best_model_path, **vars(args))\n",
    "    model.retrain = True\n",
    "    retrain_csv_logger = CSVLogger(save_dir=Path(args.log_dir) / f'{dataset_name}_{model_name}_retrain', version=timestamp)\n",
    "    callbacks = load_retrain_callbacks(args)\n",
    "    retrainer = Trainer(max_epochs=args.retrain_config['epochs'], accelerator='gpu',\\\n",
    "                      logger=retrain_csv_logger, log_every_n_steps=1, callbacks=callbacks)\n",
    "    retrainer.fit(model, datamodule=data_module)\n",
    "    retrainer.test(model, datamodule=data_module, ckpt_path='best')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
